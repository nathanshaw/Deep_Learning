{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating A small 2-Layer Neural Network\n",
    "Please note that when counting neural network layers, you dont count the input layer\n",
    "Also, please note that a[0] a[1] and a[2] are used to refer the the depth (or the layer) of the network in question\n",
    "A3[1] will be the third node in the first hidden layer \n",
    "\n",
    "## Activation Functions\n",
    "Recursive network uses sigmoid()\n",
    "Instead of using sigmoid, there are other activation functions which are popular choices. \n",
    "\n",
    "While sigmoid equal to 1 / 1 + e^-z the TanH function almost always works better. Where\n",
    "\n",
    "A = Tanh(z)\n",
    "\n",
    "Which is equal to:\n",
    "A = e^z - e^z / e^z + e^-z\n",
    "\n",
    "This is effectively a shifted sigmoid function which goes between -1 and 1 where sigmoid goes between 0 and 1\n",
    "\n",
    "For hidden layers, the use of TanH is almost always better than using sigmoid. \n",
    "This is in part because the function effectively makes the mean output of the \n",
    "function close to 0 which sort of has an effect of normalizing the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exception!!!\n",
    "For the final \"output\" layer, a sigmoid function usually works better as it outputs a number between 0 and 1 which is better for binary classificaiton problems.\n",
    "So usually TanH is used for the hidden layers while Sigmoid is used for the output layer\n",
    "\n",
    "## Problems with Both Sigmoid and TanH\n",
    "When Z is either very large or very small, the derivitive (slope) of the activation function becomes very small (almost 0) which can make training slow down\n",
    "To address this particular problem, another activation function is often used\n",
    "\n",
    "## Relu - Rectify Linear Unit\n",
    "This is perhaps the most commonly used activation function these days\n",
    "\n",
    "A = max(0, z)\n",
    "\n",
    "This has a derivitive of 0 when Z is below 0 and a derivitive of 1 when Z is positive.\n",
    "One problem with this is that when Z is equal to exactually 0, the slope is not well defined, but in practice this occurs rarely\n",
    "\n",
    "## Leaky Relu \n",
    "\n",
    "A = max(0.01*z, z)\n",
    "\n",
    "Very similar to Relu, but produces a very small derivitive when Z is positive instead of 0\n",
    "Usually performs better than Relu in practice\n",
    "\n",
    "The advantages of using both the Leaky Relu and standard Relu functions are they allow for the network to train much faster than other activation functions\n",
    "\n",
    "## A linear activation function (also known as an identity activation function) \n",
    "Will be equal to :\n",
    "\n",
    "A = Z\n",
    "\n",
    "This does not work and can not be used for the hidden layers of a network as this would defeat the whole purpose of those layers\n",
    "\n",
    "One use-case is to use this in the final output layer when the network is attempting to calculate a real number (such as the price of a house for instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Weights\n",
    "In neural networks it is important to randomly initialize the weights (unlike in logistic regression where the weights can be initialized as 0)\n",
    "\n",
    "You are able to initialize the bias values as 0 and that is fine, but not the weights\n",
    "\n",
    "The problem is that it will make it so your A1[1] and A2[1] values are the same (as they are calculating the same thing)\n",
    "\n",
    "Furthermore, when calculating the derivitives, dz1[1] will be equal to dz2[1]. \n",
    "\n",
    "So effectively, you are flattening all your nodes within that layer to a single node. The nodes will be refered to as being **symetric**\n",
    "\n",
    "My intuition is that this would be true if the weights were all assigned the same value (say 0.25 or 1.0) as well as with 0\n",
    "\n",
    "#### This is called the **symetry breaking** problem\n",
    "\n",
    "Typically what you want to do is generate the weights using a noise function and then multiply the value by a small number so the weights are close to 0, but not 0 and different form each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00882016  0.01501583]\n",
      " [-0.00329492  0.00791642]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = np.random.randn(2,2) * 0.01\n",
    "print(w1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool, but why multiple by 0.01 and not a number like 10 or 100?\n",
    "\n",
    "Well, because we are usually using a sigmoid or TanH function, we want our initial weights to be close to 0 as that is where the slope is the greatest and therefore the network will train faster initially than if the slope was close to 0\n",
    "\n",
    "There are times when other values can work better. \n",
    "\n",
    "## Dimensions of Matrices\n",
    "\n",
    "For any variable foo, the derivitivive of foo (dfoo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2)\n",
    "y = np.sum(x, axis=0, keepdims=True)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.15001352 -0.10311311]\n",
      "  [ 0.95117643 -0.12162957]]\n",
      "\n",
      " [[ 0.57263723 -0.12357659]\n",
      "  [ 0.83648332  1.61872106]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(8, 1)\n",
    "y = x.reshape(2, 2, 2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3, 4)\n",
    "b = np.random.randn(1, 4)\n",
    "c = a + b\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "d = np.random.randn(1, 3)\n",
    "e = np.random.randn(3, 3)\n",
    "f = d * e\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 45)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(12288, 150)\n",
    "b = np.random.randn(150, 45)\n",
    "c = np.dot(a, b)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
