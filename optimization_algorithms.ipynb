{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "As deep learning is an empirical process that requires many iterations and much experimentation it is important to be able to train networks quickly as this expedites the development process.\n",
    "\n",
    "Vectorization is a MUST DO optimization practice that allows us to write code without explicit for loops as we can perform matrix and vector math instead.\n",
    "\n",
    "### Mini-batch gradient descent\n",
    "Instead of running the entire training set through the network you instead allow graidient descent to happen after the model processes a portion (or batch) of the training set. This is particularally useful for when you have a large data set (say over a million training examples). \n",
    "\n",
    "The smaller subset of the training data are called **mini-batches**\n",
    "\n",
    "The notation for distinguishing between the different mini batches we use:\n",
    "\n",
    "x{1}, x{2}, x{l}\n",
    "\n",
    "y{1}, y{2}, y{l}\n",
    "\n",
    "Using mini-batch gradient descent trains the network much quicker and comes with few downsides. This approach is almost always used when training networks on large datasets.\n",
    "\n",
    "When plotting the cost function of mini-batch gradient descent the cost will contain much more noise than when using Batch gradient descent as each mini-batch is completely new data.\n",
    "\n",
    "#### Choosing the Mini-Batch size\n",
    "When the mini-batch size is equal to m you are effectively implementing a Batch gradient descent\n",
    "\n",
    "When the mini-batch size is equal to 1 you are implementing **stochastic gradient descent** \n",
    "- this method never converges, but moves around with noise constantly\n",
    "- this approach does not really result in an increase in network training speed and provides less oppertunity to vectorize math (you need for loops)\n",
    "\n",
    "SGD leads to many oscillations to reach convergence, but each step is a lot faster to compute for SGD than it is for GD, as it uses only one training example (vs. the whole batch for GD).\n",
    "- Over the number of iterations\n",
    "- Over the  ğ‘š training examples\n",
    "- Over the layers (to update all parameters, from  (ğ‘Š[1],ğ‘[1]) to  (ğ‘Š[ğ¿],ğ‘[ğ¿]))\n",
    "\n",
    "- When implementing the mini-batching you want to randomly shuffle your X and Y data so that X(1) is moved to the same new position as Y(1)\n",
    "- Next, partician the shuffled (X, Y) values into min-batches of size mini_batch_size\n",
    "- It is likely that the number of taining examples does not evenly divide by mini_batch_size. But this is not a problem as the last batch will simply be smaller than the prior batches\n",
    "\n",
    "**Note also that implementing SGD requires 3 for-loops in total**\n",
    "\n",
    "If you have a small training set (< 2000): use Batch gradient descent\n",
    "Typical mini-batch sizes are a power of 2 of 64, 128, 256, 512, (1024 sometimes, but it is more rare)\n",
    "**MAKE SURE THE MINI BATCH FITS WITHIN CPU/GPU MEMORY OR ELSE THE PERFORMANCE WILL TANK**\n",
    "\n",
    "## Exponentially Weighted (moving) Averages\n",
    "You calculate an average for each point in the dataset using some weight.\n",
    "\n",
    "- theta = data points\n",
    "- Vt = weighted average of datapoint t\n",
    "- beta = the weight value\n",
    "\n",
    "Vt = (beta) * V(t-1) + (1 - beta) * theta(t)\n",
    "\n",
    "If beta = 0.9:\n",
    "\n",
    "V1 = 0.9 * V0 + 0.1(theta1)\n",
    "\n",
    "V2 = 0.9 * V1 + 0.1(theta2)\n",
    "\n",
    "You are basically limiting the amount that the current value contributes to the average by comparing it to the values which came before to create a localized average.\n",
    "\n",
    "This takes very little memory, and with quite efficent and easy to implement.\n",
    "\n",
    "This is not the most accurate way to calculate the average, but it is computationally cheep and generally provides pretty good results.\n",
    "\n",
    "### Choosing values for Beta\n",
    "0.9 is the most common value used for beta\n",
    "\n",
    "**IMPORTANT** Vt provides an approximate average over 1/(1-beta) days\n",
    "- With a beta value of 0.9 this equates to 1/(1-0.9) = 1/0.1 = 10\n",
    "- With a beta value of 0.95 this equates to 1/(1-0.95) = 1/0.05 = 20\n",
    "- With a beta value of 0.75 this equates to 1/(1-0.75) = 1/0.25 = 4\n",
    "\n",
    "The problem with using a high beta value is that it will effectively right-shift the average away from the data.\n",
    "\n",
    "### Bias correction for Weighted Averages\n",
    "A problem with weighted averages is that until you feed 1 / (1 - beta) data points into to algorithm you will be calculating a lower average than what is accurate (as V0 is initialized as 0)\n",
    "\n",
    "**PERSONAL NOTE** I feel like this can be avoided by initializing V0 as the value of theta1.... I wonder if that is correct\n",
    "\n",
    "To combat the bia issue for the weighted average you can apply this equasion to Vt:\n",
    "\n",
    "Vt = Vt / (1 - beta^t)\n",
    "\n",
    "## Gradient descent with momentum\n",
    "Compute an exponentially weighted average of your gradients and then apply that instead of the unweighted gradients\n",
    "\n",
    "On iteration t: compute dw and db on current mini-batch\n",
    "\n",
    "- Vdw = (beta * Vdw) + ((1 - beta) * dw)\n",
    "- Vdb = (beta * Vdb) + ((1 - beta) * db)\n",
    "- W = W - learning_rate * Vdw\n",
    "- b = b - learning_rate * Vdb\n",
    "\n",
    "One way to conceptualize how the different parts of this function works is if you think of the cost function as tring to find the lowest points in a bowl and the current cost function is a ball. When using this analogy, dW and db can be thought of as the acceleration of the ball while Vdb functions as the velocity of the ball while beta serves as the friction that prevents the ball from moving too fast\n",
    "\n",
    "### Implementation\n",
    "Compute dW, db on the current mini-batch\n",
    "\n",
    "VdW = beta * Vdw + ((1 - beta) * dW)\n",
    "Vdb = beta * Vdb + ((1 - beta) * db)\n",
    "W = W - (learning_rate * Vdw)\n",
    "b = b - (learning_rate * Vdb)\n",
    "\n",
    "Hyperparameters: learning_rate, beta\n",
    "\n",
    "**PLEASE NOTE** That usually when gradient decent with momemtum is used bia correction does not occur. This is especially true with lower values of beta such as 0.9 as it only takes around ten iterations before there is no bias\n",
    "\n",
    "## RMSprop (Root Means Square)\n",
    "Has a similar effect to Momentum by reducing the oscilations \n",
    "On iteration t: computer dw, db on current mini-batch\n",
    "\n",
    "epsilon = 10^-8\n",
    "\n",
    "Sdw = (beta * Sdw) + ((1 - beta) * dw^2) (element wide square)\n",
    "Sdb = (beta * Sdb) + ((1 - beta) * db^2)\n",
    "\n",
    "W = W - (alpha * (dw/np.sqrt(Sdw + epsilon)))\n",
    "\n",
    "**IMPORTANT** One huge advantage of this approach is that it allows you to use a higher learning rate than otherwise would be possible.\n",
    "**HISTORY** RMSprop was not introduced to the DeepLearning community through an academic paper, but instead through a coursera course =)\n",
    "\n",
    "## Combining RMSprop and Momentum (Adam optimization algorithm)\n",
    "\n",
    "Adam = adaptive moment estimation\n",
    "\n",
    "Vdw = 0, Sdw = 0, Vdb = 0, Sdb = 0\n",
    "\n",
    "epsilon = 10^-8\n",
    "\n",
    "On iteration t:\n",
    "- compute dw, db using current mini-batch\n",
    "- Vdw = beta1 * Vdw + ((1 - beta1) * dw)    <--- momentum implementation\n",
    "- Vdb = beta1 * Vdb + ((1 - beta1) * db)    <--- momentum implementation\n",
    "- Sdw = beta2 * Sdw + ((1 - beta2) * dw^2)  <--- RMSprop implementation\n",
    "- Sdb = beta2 * Sdb + ((1 - beta2) * db^2)  <--- RMSprop implementation\n",
    "- Vdw(corrected) = Vdw / (1 - beta2^t)\n",
    "- Vdb(corrected) = Vdb / (1 - beta2^t)\n",
    "- Sdw(corrected) = Sdw / (1 - beta2^t)\n",
    "- Sdb(corrected) = Sdb / (1 - beta2^t)\n",
    "- W = W - (learning_rate * (Vdw(corrected) / np.sqrt(Sdw(corrected) + epsilon)))\n",
    "- b = b - (learning_rate * (Vdb(corrected) / np.sqrt(Sdb(corrected) _ epsilon)))\n",
    "\n",
    "### Hyper-parameter choices\n",
    "learning_rate = needs to be tuned\n",
    "beta1 = 0.9 <--- to calculate dw\n",
    "beta2 = 0.999 <--- to calculate dw^2\n",
    "epsilon = 10^-8 <---- this specific value does not matter too much\n",
    "\n",
    "## Learning Rate Decay\n",
    "When you use the same learning rate for all batches and iterations through the network you might run into an issue where the cost function will not converge on the optima, when the model gets close to converging, it is unable to because the learning rate is too high to fully optimize the cost function.\n",
    "- 1 epoch is one pass through the data (processing all mini-batches in the training set)\n",
    "- learning_rate0 is the unmodified hyperparamter learning_rate\n",
    "- learning_rate = 1 / (1 + decayRate * epochNumber) * learning_rate0\n",
    "\n",
    "An alternate implementaiton of learning rate decay called exponential learning rate decay is implemented as follows:\n",
    "- learning_rate = 0.95^epochnum * learning_rate\n",
    "\n",
    "Another alternate approach is:\n",
    "- learning_rate = k / np.sqrt(epochnum) * learning_rate\n",
    "\n",
    "Another approach is:\n",
    "- t is the minibatch number\n",
    "- learning_rate = k / np.sqrt(t) * learning_rate0\n",
    "\n",
    "The discrete staircase approach will half the learning rate after a set number of mini-batches are processed\n",
    "\n",
    "Another method is manual and is used when a network takes many hours or days to train. For this approach you keep an eye on the output of the model as it trains and manually adjust the training rate as the network trains.\n",
    "\n",
    "## Problems with local optima\n",
    "This is mostly a problem with lower dimentional spaces, it is very unlickely in high dimensions that all dimensions will converge into a local optima instead of a global optima. Pleateaus can drastically slow down the model training however.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
