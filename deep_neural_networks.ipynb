{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "Logistic regression can be considered to be a single layer neural network without a hidden layer\n",
    "\n",
    "Deep neural networks typically have 3 or more hidden layers\n",
    "\n",
    "\n",
    "### How Deep does my network need to be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations for Deep Networks\n",
    "\n",
    "**L** is the number of layers in the network\n",
    "\n",
    "**n[l]** is the number of units in layer l\n",
    "\n",
    "n[1] is the number of nodes in the first layer (After the input data)\n",
    "\n",
    "n[0] is equal to Nx or the number of input features put into the network\n",
    "\n",
    "X is the input features put into the equasion\n",
    "\n",
    "X = a[0]\n",
    "\n",
    "Yhat = a[l]\n",
    "\n",
    "m is the number of training examples\n",
    "\n",
    "a[l] = output from the **l**-th layer\n",
    "\n",
    "z[l] = input to the **l**-th layer\n",
    "\n",
    "## Parameter Dimensions\n",
    "\n",
    "It is critical to understand the expected dimensions of your parameters when coding neural networks. The source of many bugs, especially for beginners is imporoper vector and matrix sizes. Here are some guidelines for how to think about the dimensions of various parameters\n",
    "\n",
    "### Z and A\n",
    "z[l], a[l] = (n[l], 1)\n",
    "\n",
    "Z[l], A[l] = (n[l], m)\n",
    "\n",
    "The dimensions of z[l] and a[l] are (n[l], 1), where n[l] is the number of neurons in the l-th layer. This is because z[l] and a[l] represent the activation of each neuron in the l-th layer, and a single neuron's activation is typically represented as a column vector with n[l] rows.\n",
    "\n",
    "The dimensions of Z[l] and A[l] are (n[l], m), where m is the number of examples in the dataset. This is because Z[l] and A[l] represent the activations of all neurons in the l-th layer for all examples in the dataset. In other words, each column of Z[l] and A[l] represents the activation of all neurons in the l-th layer for a single example in the dataset.\n",
    "\n",
    "It's worth noting that these dimensions may vary depending on the specific architecture of the neural network and the conventions used in its implementation. However, the dimensions provided are common conventions that are often used in neural network literature.\n",
    "\n",
    "### How About W?\n",
    "\n",
    "W[l] = (n[l], n[l - 1])\n",
    "\n",
    "The shape of W[l] is (n[l], n[l-1]), where n[l] is the number of neurons in the l-th layer, and n[l-1] is the number of neurons in the previous (l-1)-th layer. This is because each neuron in the l-th layer is connected to all neurons in the previous (l-1)-th layer via a weight parameter, resulting in a weight matrix of shape (n[l], n[l-1]).\n",
    "\n",
    "### X \n",
    "\n",
    "X = (n[0], 1)\n",
    "\n",
    "X corresponds to the input data and therefore the first layer in the network. Therefore, it will have n[0] rows where n[0] corresponds with the number of nodes in the input layer. \n",
    "\n",
    "### b\n",
    "\n",
    "b[l] = (n[l], 1)\n",
    "\n",
    "Likewise, as the bias b[l] must be added to the output of each node it contains n[l] rows where each row represents the bias for a particular node in the layer.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Please note that x in this equasion is the same as A[0], effitively making the lines of code identical\n",
    "\n",
    "X: Z[1] = W[1]x + b[1] \n",
    "A[1] = g[1](Z[1])\n",
    "Z[2] = W[2]A[1] + b[2]\n",
    "A[2] = g[2](Z[2])\n",
    "Z[3] = W[3]A[2] + b[3]\n",
    "A[3] = g[3](Z[3])\n",
    "etc...\n",
    "\n",
    "In a general abstract sense, the equasion for each layer is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
