{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow \n",
    "TensorFlow is an open-source machine learning library developed by Google Brain Team. It provides a comprehensive ecosystem of tools, libraries, and resources for building, training, and deploying machine learning models. TensorFlow is designed to be flexible, efficient, and scalable, making it suitable for various applications ranging from research to production.\n",
    "\n",
    "TensorFlow supports deep learning, traditional machine learning, and other numerical computation tasks, thanks to its flexible architecture. It allows developers to create computation graphs, which can be executed efficiently on CPUs, GPUs, and TPUs (Tensor Processing Units). The library has APIs available in multiple languages, such as Python, C++, and Java, with Python being the most popular and widely used.\n",
    "\n",
    "TensorFlow also offers tools for visualization and debugging, like TensorBoard, which helps users monitor and analyze their model's performance. Furthermore, TensorFlow Extended (TFX) provides an end-to-end platform for deploying machine learning models in production environments.\n",
    "\n",
    "In summary, TensorFlow is a versatile, open-source machine learning library with a wide range of capabilities for building, training, and deploying models across various platforms and devices. It offers a rich ecosystem of tools and resources to cater to the needs of researchers, developers, and businesses alike.\n",
    "\n",
    "## Why Use TensorFlow over other Machine Learning Frameworks?\n",
    "Advantages:\n",
    "\n",
    "- Flexibility: TensorFlow provides a flexible architecture that allows you to define and deploy custom computation graphs and operations, making it suitable for a wide range of machine learning tasks and research.\n",
    "- Scalability: TensorFlow is designed to scale across multiple devices (CPU, GPU, and TPU) and platforms (desktop, server, and mobile), enabling efficient training and deployment of large-scale models.\n",
    "- Ecosystem: TensorFlow has a rich ecosystem of libraries, tools, and resources that makes it easier to develop, train, and deploy machine learning models. This includes TensorFlow Extended (TFX) for end-to-end model deployment, TensorBoard for visualization, and TensorFlow Hub for pre-trained models.\n",
    "- Community and Support: TensorFlow has a large and active community, which provides extensive documentation, tutorials, and resources. The backing from Google ensures continuous development, updates, and improvements.\n",
    "- Multi-language support: TensorFlow offers APIs in multiple languages, such as Python, C++, Java, and others. This allows developers with different language preferences to utilize TensorFlow effectively.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Learning curve: TensorFlow has a steeper learning curve compared to some other frameworks, especially for beginners. The computation graph-based approach, while powerful, might require more time to understand and master.\n",
    "- Verbosity: TensorFlow's API can be more verbose than some other frameworks, which might lead to longer and more complex code. However, this has been partially addressed with the introduction of the higher-level Keras API.\n",
    "- Debugging: Debugging TensorFlow models can be challenging due to the symbolic nature of the computation graph, which separates the graph definition from its execution.\n",
    "- Performance: While TensorFlow is generally efficient and scalable, some specific use cases might be better served by other frameworks that offer better performance for particular tasks or hardware configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 10:22:56.722849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 10:23:09.208226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# weight\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "# optimization\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def train_step():\n",
    "    # our cost function\n",
    "    # creates a tf.GradientTape context manager and\n",
    "    #  assigns it to the variable tape. tf.GradientTape \n",
    "    # is used to record operations for automatic differentiation, \n",
    "    # i.e., it helps compute the gradients of a function with \n",
    "    # respect to its variables. In this case, it will be used \n",
    "    # to compute the gradient of cost with respect to w.\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = w ** 2 - 10 * w + 25\n",
    "    trainable_variables = [w]\n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    \"\"\"zip is a built-in function in Python that allows you to \n",
    "    combine multiple iterables (e.g., lists, tuples, sets) \n",
    "    element-wise, forming a new iterable of tuples. Each tuple \n",
    "    contains the elements from the input iterables that have the \n",
    "    same index.\"\"\"\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999931>\n"
     ]
    }
   ],
   "source": [
    "train_step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.19994053>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.039641>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000138>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000014>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000014>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000014>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000014>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000014>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0000014>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    train_step()\n",
    "    if i % 100 == 0:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.000001>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "x = np.array([1.0, -10.0, 25.0], dtype=np.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def training(x, w, optimizer):\n",
    "    def cost_fn():\n",
    "        return x[0] * w ** 2 + x[1] * w + x[2]\n",
    "    for i in range(1000):\n",
    "        optimizer.minimize(cost_fn, [w])\n",
    "    return w\n",
    "\n",
    "w = training(x, w, optimizer)\n",
    "print(w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a h5py file?\n",
    "An h5py file is a file format used in Python for storing large amounts of numerical data, such as arrays or datasets, efficiently and hierarchically. It is based on the Hierarchical Data Format version 5 (HDF5), which is a widely used data storage format in scientific computing and other fields that require handling large datasets.\n",
    "\n",
    "The h5py library in Python provides an easy-to-use, high-level interface for working with HDF5 files. With h5py, you can read and write data to and from HDF5 files, organize data into groups, and handle complex data types, such as variable-length strings and arrays.\n",
    "\n",
    "HDF5 files are particularly useful for working with large numerical datasets that do not fit into memory, as they allow for efficient reading and writing of data in chunks. This makes them a popular choice for storing and sharing data in fields like machine learning, high-performance computing, and scientific simulations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Generators in Python?\n",
    "Generators in Python are a type of iterator, which allow you to iterate over a sequence of values lazily. Instead of generating all the values at once and storing them in memory, generators produce values on-the-fly, one at a time, as you iterate through them. This can be highly efficient for large datasets or sequences that would otherwise consume too much memory if generated at once.\n",
    "\n",
    "Generators are created using a special kind of function called a generator function. These functions use the yield keyword instead of return to produce values. When a generator function is called, it returns a generator object, which can then be iterated over using a for loop or the next() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def count_up_to(max_value):\n",
    "    count = 1\n",
    "    while count <= max_value:\n",
    "        yield count\n",
    "        count += 1\n",
    "\n",
    "# Using the generator function\n",
    "for number in count_up_to(5):\n",
    "    print(number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax of generators in Python revolves around two main components: the generator function and the yield keyword. Let's break down each of these components:\n",
    "\n",
    "Generator function: A generator function is a special type of function in Python that, instead of returning a value and terminating, returns a generator object. This generator object can be iterated over, producing a sequence of values on-the-fly, one at a time. To define a generator function, you use the def keyword, just like with any other Python function. The main difference between a generator function and a regular function is the use of the yield keyword instead of return.\n",
    "\n",
    "yield keyword: The yield keyword is used in generator functions to produce a value and temporarily pause the execution of the function. When the generator is iterated over, it resumes the execution from where it left off, using the current state (i.e., the values of local variables) at the time of yielding. This allows the generator to produce values one by one as they are requested, rather than generating all the values at once and storing them in memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The set() function\n",
    "n Python, the set() function is used to create a set, which is an unordered collection of unique elements. While the set() function itself is a built-in Python function and is not specifically related to TensorFlow, it can be used in conjunction with TensorFlow to solve certain problems or perform specific tasks.\n",
    "\n",
    "For example, you might use the set() function in a TensorFlow project to:\n",
    "\n",
    "Remove duplicate elements: You can use the set() function to remove duplicate elements from a list, such as a list of class labels or feature names in a machine learning dataset. TensorFlow can then use this cleaned list for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "labels = [1, 2, 3, 2, 4, 1, 5]\n",
    "unique_labels = set(labels)\n",
    "print(unique_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform set operations: You can use the set() function along with other set operations like union, intersection, and difference to process and manipulate data in your TensorFlow project. These operations can be useful when you need to compare or combine different sets of data in your machine learning or deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3}\n"
     ]
    }
   ],
   "source": [
    "set_a = {1, 2, 3}\n",
    "set_b = {3, 4, 5}\n",
    "intersection = set_a.intersection(set_b)  # Output: {3}\n",
    "print(intersection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the set() function is not directly tied to TensorFlow, it can be a valuable tool for data preprocessing and manipulation when working with TensorFlow or any other machine learning and deep learning library. Keep in mind that the set() function and its related operations are specific to Python's built-in data structures and are not designed to work directly on TensorFlow tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map() and TensorFlow\n",
    "TensorFlow datasets: TensorFlow provides the Dataset class, which is a high-level abstraction for handling data input pipelines. When you want to apply a transformation to each element in a TensorFlow dataset, you would use the map() method. The map() method takes a function as its argument and applies it to every element in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "<MapDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def double(x):\n",
    "    return x * 2\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])\n",
    "\n",
    "# Apply the 'double' function to each element in the dataset using the map method\n",
    "transformed_dataset = dataset.map(double)\n",
    "\n",
    "print(dataset)\n",
    "print(transformed_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with TensorFlow datasets, you should use the map() method to apply a transformation to each element, whereas with NumPy arrays, you can apply transformations directly using element-wise operations or Python's built-in map() function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are One Hot Encodings?\n",
    "One Hot Encoding is a technique used in machine learning and data preprocessing to represent categorical variables as binary vectors. It is particularly useful when working with algorithms that expect numerical input features but encounter categorical data instead.\n",
    "\n",
    "In One Hot Encoding, each category in a categorical variable is represented as a separate binary feature or dimension. These binary features take the value of 1 when the category is present (or \"hot\") and 0 when it is not present (or \"cold\"). This way, the categorical data is converted into a numerical format that machine learning algorithms can better understand and process.\n",
    "\n",
    "Here's an example to illustrate One Hot Encoding:\n",
    "\n",
    "Suppose you have a dataset with a categorical feature called \"Color\" that has three distinct categories: \"Red,\" \"Green,\" and \"Blue.\" Using One Hot Encoding, you would create three separate binary features, one for each category:\n",
    "\n",
    "- Red: [1, 0, 0]\n",
    "- Green: [0, 1, 0]\n",
    "- Blue: [0, 0, 1]\n",
    "\n",
    "Now, if you have a data point with the \"Color\" value \"Green,\" you would represent it as the vector [0, 1, 0] in the transformed dataset.\n",
    "\n",
    "One Hot Encoding has some advantages, such as:\n",
    "\n",
    "It allows machine learning algorithms to work with categorical data that would otherwise be incompatible with numerical input features.\n",
    "It prevents the introduction of false assumptions or ordinal relationships between categories that do not exist, which might happen if the categorical data were simply replaced with integer values.\n",
    "However, One Hot Encoding also has some disadvantages:\n",
    "\n",
    "It can lead to a large increase in the number of features, especially when dealing with categorical variables with many distinct categories. This can cause increased memory usage and longer training times.\n",
    "It does not capture any inherent relationship between categories if such a relationship exists.\n",
    "Many machine learning libraries, such as scikit-learn in Python, provide built-in functions to perform One Hot Encoding, making it easy to apply this technique to your dataset as part of your preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
