{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "Transformers are a type of deep learning architecture introduced by Vaswani et al. in the 2017 paper \"Attention Is All You Need\". Transformers have become very popular in natural language processing (NLP) and other sequence-to-sequence tasks due to their high performance and scalability.\n",
    "\n",
    "Transformers are primarily built around the self-attention mechanism, which allows the model to weigh the importance of different elements in a sequence when processing a particular element. This allows Transformers to effectively capture long-range dependencies and contextual information, which is crucial for many NLP tasks.\n",
    "\n",
    "The Transformer architecture consists of two main components: the encoder and the decoder. Both the encoder and decoder are composed of multiple identical layers, with each layer consisting of several sub-layers such as multi-head self-attention, positional feed-forward neural networks, and layer normalization.\n",
    "\n",
    "Encoder: The encoder processes the input sequence and generates a continuous representation for each element in the sequence. It consists of a stack of layers, each containing a multi-head self-attention mechanism and a position-wise feed-forward network. The encoder is responsible for learning the contextualized representation of the input sequence.\n",
    "\n",
    "Decoder: The decoder generates the output sequence using the continuous representations generated by the encoder. It also consists of a stack of layers, each containing a multi-head self-attention mechanism, a position-wise feed-forward network, and an additional cross-attention layer that attends to the encoder's output. The decoder is responsible for generating the output sequence based on the input sequence and the learned contextual information.\n",
    "\n",
    "Transformers use positional encoding to inject information about the position of elements in the sequence since the self-attention mechanism is inherently permutation-invariant.\n",
    "\n",
    "Transformers have become the foundation for many state-of-the-art NLP models, such as BERT, GPT, and T5, and have achieved top performance on a wide range of tasks, including machine translation, sentiment analysis, named entity recognition, and question-answering."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
