{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "Logistic regression can be considered to be a single layer neural network without a hidden layer\n",
    "\n",
    "Deep neural networks typically have 3 or more hidden layers\n",
    "\n",
    "\n",
    "### How Deep does my network need to be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations for Deep Networks\n",
    "\n",
    "**L** is the number of layers in the network\n",
    "\n",
    "**n[l]** is the number of units in layer l\n",
    "\n",
    "n[1] is the number of nodes in the first layer (After the input data)\n",
    "\n",
    "n[0] is equal to Nx or the number of input features put into the network\n",
    "\n",
    "X is the input features put into the equasion\n",
    "\n",
    "X = a[0]\n",
    "\n",
    "Yhat = a[l]\n",
    "\n",
    "m is the number of training examples\n",
    "\n",
    "a[l] = output from the **l**-th layer\n",
    "\n",
    "z[l] = input to the **l**-th layer\n",
    "\n",
    "J() = the cost function for the network\n",
    "\n",
    "## Parameter Dimensions\n",
    "\n",
    "It is critical to understand the expected dimensions of your parameters when coding neural networks. The source of many bugs, especially for beginners is imporoper vector and matrix sizes. Here are some guidelines for how to think about the dimensions of various parameters\n",
    "\n",
    "### Z and A\n",
    "**z[l], a[l] = (n[l], 1)**\n",
    "\n",
    "**Z[l], A[l] = (n[l], m)**\n",
    "\n",
    "The dimensions of z[l] and a[l] are (n[l], 1), where n[l] is the number of neurons in the l-th layer. This is because z[l] and a[l] represent the activation of each neuron in the l-th layer, and a single neuron's activation is typically represented as a column vector with n[l] rows.\n",
    "\n",
    "The dimensions of Z[l] and A[l] are (n[l], m), where m is the number of examples in the dataset. This is because Z[l] and A[l] represent the activations of all neurons in the l-th layer for all examples in the dataset. In other words, each column of Z[l] and A[l] represents the activation of all neurons in the l-th layer for a single example in the dataset.\n",
    "\n",
    "It's worth noting that these dimensions may vary depending on the specific architecture of the neural network and the conventions used in its implementation. However, the dimensions provided are common conventions that are often used in neural network literature.\n",
    "\n",
    "**dZ[l], dA[l] = (n[l], m)**\n",
    "\n",
    "### How About W?\n",
    "\n",
    "**W[l] = (n[l], n[l - 1])**\n",
    "\n",
    "The shape of W[l] is (n[l], n[l-1]), where n[l] is the number of neurons in the l-th layer, and n[l-1] is the number of neurons in the previous (l-1)-th layer. This is because each neuron in the l-th layer is connected to all neurons in the previous (l-1)-th layer via a weight parameter, resulting in a weight matrix of shape (n[l], n[l-1]).\n",
    "\n",
    "### X \n",
    "\n",
    "**X = (n[0], 1)**\n",
    "\n",
    "X corresponds to the input data and therefore the first layer in the network. Therefore, it will have n[0] rows where n[0] corresponds with the number of nodes in the input layer. \n",
    "\n",
    "### b\n",
    "\n",
    "**b[l] = (n[l], 1)**\n",
    "\n",
    "Likewise, as the bias b[l] must be added to the output of each node it contains n[l] rows where each row represents the bias for a particular node in the layer.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why go Deep? Intuition about Networks\n",
    "Suppose we are building a neural network for face detection. The network can be conceptualized as a series of layers, where each layer processes the output of the previous layer to extract increasingly complex features from the input image.\n",
    "\n",
    "In the initial layers, the network may learn to identify simple patterns such as edges or corners in the image by examining a small region of pixels at a time. As the information is passed to the subsequent layers, the network begins to combine these simple patterns to form more complex features, such as face-like shapes with rough approximations of eyes, noses, or ears.\n",
    "\n",
    "In the later layers, the network can further refine these features and start to recognize different face configurations by gathering together these features in a hierarchical manner. By examining a larger and larger part of the original image with each subsequent layer, the network can capture increasingly complex and abstract representations of the input.\n",
    "\n",
    "It's worth noting that this process of feature extraction is not limited to face detection but is a general principle that underlies the operation of many types of neural networks. Additionally, the specific architectures and techniques used to implement these networks can vary widely and can have a significant impact on their performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Theory and Deep Learning\n",
    "Circuit theory provides a mathematical framework for understanding the behavior of neural networks with many hidden layers, and can help explain why deep networks often outperform shallow networks with a similar number of parameters.\n",
    "\n",
    "One key insight from circuit theory is that adding more layers to a neural network increases the effective \"depth\" of the composite function that the network computes. Each layer of the network applies a nonlinear transformation to its inputs, which can be viewed as a type of filter that extracts useful features from the input data. By stacking many of these filters together, a deep network can extract increasingly complex and abstract features from the input, which can help improve the network's ability to learn complex patterns and relationships.\n",
    "\n",
    "In contrast, a shallow network with a similar number of parameters would need to rely on fewer, more complex filters to extract useful features, which can lead to overfitting or underfitting of the training data. By adding more layers, a deep network can learn a more hierarchical representation of the input data, which can help it generalize better to unseen data and improve its performance on a variety of tasks.\n",
    "\n",
    "Additionally, circuit theory provides insights into the optimization of deep networks, which can be challenging due to the large number of parameters and complex, highly nonlinear nature of the objective function. By modeling the network as a type of electrical circuit, researchers can use techniques such as backpropagation and stochastic gradient descent to optimize the network's weights and biases, and improve its performance on a given task.\n",
    "\n",
    "In summary, circuit theory provides a theoretical foundation for understanding why deep networks with many hidden layers can outperform shallow networks with fewer hidden layers but more nodes in each layer. By modeling the network as a type of circuit, researchers can gain insights into how the network operates, how to optimize its performance, and how to design more efficient and effective network architectures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Please note that x in this equasion is the same as A[0], effitively making the lines of code identical\n",
    "\n",
    "X: Z[1] = W[1]x + b[1]\n",
    "\n",
    "A[1] = g[1](Z[1])\n",
    "\n",
    "Z[2] = W[2]A[1] + b[2]\n",
    "\n",
    "A[2] = g[2](Z[2])\n",
    "\n",
    "Z[3] = W[3]A[2] + b[3]\n",
    "\n",
    "A[3] = g[3](Z[3])\n",
    "\n",
    "etc...\n",
    "\n",
    "In a general abstract sense, the equasion for each layer is the same for each layer where\n",
    "- Input = a[l - 1]\n",
    "- Output = a[l], cache z[l], W[l], b[l]\n",
    "- Calculate Z[l] = W[l] * a[l - 1] + b[l]\n",
    "- a[l] = g[l](Z[l])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Propagation\n",
    "Similar to with the forward propagation step, for backwards propagation, you calculate the output for each layer using the same basic logic\n",
    "- Input = da[l]\n",
    "- Output = da[l - 1], dW[l], db[l]\n",
    "- dz[l] = da[l] * g[l]'(z[l])\n",
    "- dW[l] = dz[l] * a[l - 1].T\n",
    "- db[l] = dz[l]\n",
    "- da[l - 1] = W[l].T * dz[l]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding dz[l]\n",
    "In a neural network, the term \"dz[l]\" typically refers to the derivative of the activation function applied to the output of the l-th layer of the network, with respect to the input to that layer.\n",
    "\n",
    "To understand this, let's start with the basic definition of the output of a neuron in a neural network. The output of a neuron can be represented as:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**z[l] = W[l] * a[l-1] + b[l]**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where W[l] is the weight matrix for the l-th layer, a[l-1] is the output of the (l-1)-th layer (i.e., the input to the l-th layer), and b[l] is the bias vector for the l-th layer.\n",
    "\n",
    "Once the output z[l] is computed, it is typically passed through an activation function g to produce the final output of the neuron:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a[l] = g(z[l])**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the derivative of the activation function with respect to the input to the layer (i.e., dz[l]) is an important quantity for computing the gradients during backpropagation. Specifically, during backpropagation, we need to compute the gradients of the cost function with respect to the parameters of the network (i.e., the weights and biases).\n",
    "\n",
    "To compute these gradients, we use the chain rule of differentiation. The chain rule tells us that if a variable y depends on a variable x, which in turn depends on a variable z, then the derivative of y with respect to z can be computed as:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dy/dz = dy/dx * dx/dz**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a neural network, the output of the l-th layer (i.e., a[l]) depends on the input to that layer (i.e., a[l-1]), which in turn depends on the output of the (l-2)-th layer (i.e., a[l-2]), and so on. So to compute the gradients of the cost function with respect to the parameters of the network, we need to use the chain rule to compute the derivatives of the output with respect to the input for each layer.\n",
    "\n",
    "This is where dz[l] comes in. dz[l] represents the derivative of the activation function with respect to the input to the l-th layer. This quantity can be computed as:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dz[l] = dg(z[l]) / dz[l]**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where dg(z[l]) is the derivative of the activation function g with respect to its input z[l]. Once we have computed dz[l], we can use it to compute the gradients of the cost function with respect to the parameters of the network using the chain rule.\n",
    "\n",
    "In summary, dz[l] is an important quantity in computing the gradients during backpropagation in a neural network. It represents the derivative of the activation function with respect to the input to the l-th layer, and is used to compute the gradients of the cost function with respect to the parameters of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters vs Hyper-Parameters\n",
    "Parameters are things that are calculated by the algorhythm and include:\n",
    "- W\n",
    "- b\n",
    "\n",
    "Hyper-parameters are parameters which you must feed into the model which affect the values of W and b and include:\n",
    "- learning_rate (alpha)\n",
    "- number of iterations \n",
    "- number of hidden layers (n(1), n(2), etc)\n",
    "- number of nodes within layers (n[1], n[2], etc)\n",
    "- choice of activation function\n",
    "- momentum\n",
    "- mini-batch size\n",
    "- regularization parameters\n",
    "\n",
    "As they are parameters for the parameters, they are called hyper-parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does Deep Learning have to do with the brain?\n",
    "Not much really\n",
    "\n",
    "## Training / Dev / Test sets\n",
    "You dont really need a test set unless you need to ensure the model is unbiased\n",
    "Training set is always the largest\n",
    "Dev set is what you use to quickly test the performance of different network configurations\n",
    "Test set is data you want to test the performance of the model on to determine its performance \n",
    "\n",
    "## Bias and Variance\n",
    "High bias is poor performance of the training set data\n",
    "High variance is poor performance of the test set data\n",
    "\n",
    "Solutions for High Bias\n",
    "- larger network\n",
    "- longer training sessions\n",
    "- (research new NN configurations)\n",
    "\n",
    "Solutions for High Variancs\n",
    "- more training data\n",
    "- regularization\n",
    "- (research new NN configurations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
