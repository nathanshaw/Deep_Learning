{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cc14041",
   "metadata": {},
   "source": [
    "# Logistic Regressison\n",
    "Logistic Regression is a statistical method used for analyzing and predicting binary outcomes in a dataset. It is a type of generalized linear model (GLM) that uses a logistic function to model the probability of an event occurring, such as success or failure, based on one or more predictor variables. By applying a transformation called the logit function to the linear combination of these predictor variables, Logistic Regression estimates the probability of an event happening and classifies observations into two categories. It is widely used in various fields, such as medicine, social sciences, and marketing, for its simplicity, interpretability, and effectiveness in modeling relationships between binary outcomes and predictor variables.\n",
    "\n",
    "## Common Use-cases\n",
    "Logistic Regression has a wide range of applications across various domains. Some common use-cases include:\n",
    "\n",
    "* **Medical Diagnosis**: Predicting the presence or absence of a disease based on patient data, such as age, gender, and medical test results.\n",
    "\n",
    "* **Credit Scoring**: Assessing the creditworthiness of a borrower by predicting the probability of loan default based on financial and demographic information.\n",
    "\n",
    "* **Marketing**: Predicting customer behavior, such as the likelihood of purchasing a product, subscribing to a service, or responding to an advertisement, based on customer demographics and past behavior.\n",
    "\n",
    "* **Spam Detection**: Classifying emails as spam or not spam based on features like email content, sender information, and email metadata.\n",
    "\n",
    "* **Employee Attrition**: Predicting employee turnover in a company based on factors like job satisfaction, salary, and workload, to help organizations identify and address potential issues.\n",
    "\n",
    "* **Political Analysis**: Forecasting election results or predicting voter preferences based on demographic, socioeconomic, and political factors.\n",
    "\n",
    "* **Customer Churn**: Identifying the likelihood of customers discontinuing their use of a service or product, enabling businesses to target retention efforts more effectively.\n",
    "\n",
    "* **Social Sciences**: Investigating relationships between binary outcomes and explanatory variables in various fields, such as psychology, economics, and sociology.\n",
    "\n",
    "These are just a few examples, and Logistic Regression can be applied to a multitude of other problems involving binary outcomes and predictor variables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f555699",
   "metadata": {},
   "source": [
    "\n",
    "## Important Variables\n",
    "* **X**: The input data. The dimension of X is (n, m), where n is the number of input features per example and m is the number of examples in the dataset. The notation [x(1) x(2) ... x(m)] suggests that each column of X represents an example in the dataset, and each row represents a specific input feature. Therefore, the dimension of X is n rows by m columns.\n",
    "\n",
    "\n",
    "* **Y**: The target (or label values associated with the input data). It is the ground truth or observed outcomes that you want the logistic regression model to predict.  **Y** usually consists of binary values, typically represented as 0 and 1, for a two-class problem. \n",
    "\n",
    "* **Z**: The linear output of the function which is calculated as *Z = W.T * X + b*\n",
    "\n",
    "* **W**: The weight matrix.\n",
    "\n",
    "* **b**: A scalar constant added to the linear combination of features and weights. It represents the intercept of the logistic function and plays a crucial role in the model's decision boundary.\n",
    "\n",
    "* **A**: **A** is the output of the activation function. In logistic regression, the activation function used is the sigmoid function. **A** represents the predicted probabilities for each of the m training examples.\n",
    "\n",
    "* **m**: The number of training examples or the size of the training dataset. It is used to calculate the average cost over all training examples or to compute the average gradient for updating the model parameters during the optimization process.\n",
    "\n",
    "### Back-Prop Varialbes\n",
    "* **dZ**: the partial derivative of the cost function with respect to the linear output **Z**. **dZ** is used during back-propagation to compute gradients for updating the models parameters. **dZ** is essential in calculating gradients for **W** and **b** which are subsequently used to update the model's parameters during the optimization process.\n",
    "\n",
    "* **dA**: the derivative of the cost function is respect to the activation function output **A**\n",
    "\n",
    "* **dW**: represents the partial derivative of the cost function with respect to the weight parameters **W**. It is the gradient of the weights and provides information about the direction and magnitude by which the weights need to be updated to minimize the cost function. During the training process, you calculate dW and use it to update the weight parameters iteratively to converge to the optimal solution.\n",
    "\n",
    "* **g(Z)**: The derivative of the activation function (which is a sigmoid function in the case of Logistic Regression)\n",
    "\n",
    "#### W^T operation\n",
    "* **W^T** is the transpose of the weight vector **W**. The letter \"T\" denotes the transpose operation, which means the rows and columns of the matrix or vector are interchanged. In the context of logistic regression, the weight vector W contains the weights associated with each input feature.\n",
    "\n",
    "* When you see **W^T**, it means that the weight vector has been transposed from a column vector to a row vector (or vice versa). The purpose of this operation is to make matrix multiplication compatible between the weight vector and the input feature vector, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9782b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# as Both A and Y are vectors of the same length\n",
    "# we can subtract the two vectors to implement\n",
    "# A(1)-Y(1), A(2)-Y(2), for the entire length of the vectors\n",
    "\n",
    "# our input data set, for this example 100 random numbers\n",
    "X = np.random(100)\n",
    "# m is the length of our input data set\n",
    "m = len(X)\n",
    "# A is the output of the Activation function\n",
    "Y = np.zeros(m)\n",
    "\n",
    "# initialize the bias as 0\n",
    "b = 0\n",
    "\n",
    "# W is the weight matrix which contains the coefficients for each\n",
    "# predictor variable (feature) in the model\n",
    "\"\"\" \n",
    "For binary logistic regression, the weight matrix is a column\n",
    "vector with a shape of (n_features + 1, 1), where n_features \n",
    "is the number of predictor variables in the dataset. The \"+1\"\n",
    "accounts for the bias term (also known as the intercept), which \n",
    "is not associated with any specific feature but represents the \n",
    "base output when all feature values are zero.\n",
    "\n",
    "Initially, the weight matrix is often set to small random values \n",
    "or to zeros. Starting with small random values can help break the \n",
    "symmetry during the optimization process, while initializing the \n",
    "weights to zero results in a simpler, more interpretable model. \n",
    "\n",
    "The weight matrix is then iteratively updated during the training \n",
    "process using an optimization algorithm, such as gradient descent, \n",
    "to minimize the loss function.\n",
    "\"\"\"\n",
    "# Initialize with small random values\n",
    "W = np.random.random(m + 1, 1) * 0.01\n",
    "\n",
    "# Or initialize with zeros\n",
    "# W = np.zeros((m + 1, 1))\n",
    "\n",
    "##################################################################################\n",
    "\"\"\" \n",
    "Calculating A: Given a feature matrix X with dimensions (n, m), where n is the \n",
    "number of features and m is the number of training examples, and a weight vector\n",
    "W with dimensions (n, 1), you first calculate the weighted sum Z:\n",
    "Z = W^T * X + b\n",
    "Z = np.dot(wt, X) + b\n",
    "next, for Logistic Regression, a sigmoid() function is applied to Z\n",
    "A = sigmoid(Z)\n",
    "where the sigmoid function is \n",
    "sigmoid(z) = 1/ (1 + exp(-z)) \n",
    "\"\"\"\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# so A is then equal to \n",
    "# This function maps any input value z to a probability value between 0 and 1.\n",
    "A = sigmoid(Z)\n",
    "\n",
    "# the partial derivitives of the cost function as a part of the linear output predictions\n",
    "dZ = A - Y\n",
    "\n",
    "# the partial derivitaves of cost function as a component of the weights\n",
    "# calculated as an average of the cost through all weights\n",
    "dW = (1/w) * np.sum(dZ)\n",
    "\n",
    "# Now here is where some vectorization magic occurs, instead of using for loops,\n",
    "# which are huge No No's for Deep Learning we can use our numpy vector operations to\n",
    "# drastically reduce the runtime of the program\n",
    "\n",
    "# TODO - lol, actually not sure if this line of code is correct...\n",
    "W = W - sigmoid(dW) \n",
    "\n",
    "# TODO - same with this one...\n",
    "b = (1/m) * np.sum(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde3773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a56f0db8",
   "metadata": {},
   "source": [
    "## Only One For Loop\n",
    "For the most efficient implementation of gradient descent, the only for loop should be\n",
    "the one which controls the total number of optimizations are run for the regression routine. The rest of the code should take advantage of vectorization to optimise the calculation times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84335cbb",
   "metadata": {},
   "source": [
    "## Element wise and Matrix Multi - The Differences\n",
    "\n",
    "Element-wise multiplication and matrix multiplication are two different types of mathematical operations on arrays in linear algebra.\n",
    "\n",
    "Element-wise multiplication, also called Hadamard product, is performed by multiplying corresponding elements of two arrays or matrices of the same shape. The resulting array will have the same shape as the input arrays, and each element in the output array is the product of the corresponding elements in the input arrays. In NumPy, this operation is performed using the * operator.\n",
    "\n",
    "For example, given two arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Copy code\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "c = a * b\n",
    "c = np.array([4, 10, 18])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a845d156",
   "metadata": {},
   "source": [
    "\n",
    "Matrix multiplication, on the other hand, is a more complex operation, where two matrices are multiplied together to produce a new matrix. The key requirement is that the number of columns in the first matrix must be equal to the number of rows in the second matrix. The resulting matrix will have the number of rows of the first matrix and the number of columns of the second matrix. In NumPy, this operation is performed using the dot() function or the @ operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5412127",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "C = np.dot(A, B)\n",
    "C = np.array([[19, 22], [43, 50]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4effa96",
   "metadata": {},
   "source": [
    "\n",
    "For example, given two matrices:\n",
    "\n",
    "\n",
    "Element-wise multiplication multiplies corresponding elements of two arrays or matrices, while matrix multiplication multiplies two matrices.\n",
    "Element-wise multiplication results in an array of the same shape as the input arrays, while matrix multiplication results in a matrix with the number of rows of the first matrix and the number of columns of the second matrix.\n",
    "Element-wise multiplication is performed using the * operator in NumPy, while matrix multiplication is performed using the dot() function or the @ operator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
