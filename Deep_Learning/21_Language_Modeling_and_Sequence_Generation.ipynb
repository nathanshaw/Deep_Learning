{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "A language model in machine learning is a model that predicts the probability of a sequence of words in a given language. Essentially, it learns the patterns and rules of a language and uses them to generate or predict text.\n",
    "\n",
    "Recurrent neural networks (RNNs) are commonly used for language modeling. RNNs are a type of neural network that are able to handle sequential data by maintaining an internal memory state that captures information about the previous inputs in the sequence. By training an RNN on a large corpus of text, it can learn to predict the likelihood of a given sequence of words and generate new text.\n",
    "\n",
    "In language modeling, the RNN is trained to predict the next word in a sequence, given the previous words in the sequence. The input to the RNN is a sequence of words, and the output is a probability distribution over the next word in the sequence. The RNN is trained using a corpus of text, and the goal is to minimize the difference between the predicted probability distribution and the true probability distribution over the next word in the sequence.\n",
    "\n",
    "Once the language model is trained, it can be used for a variety of tasks, such as generating new text, machine translation, and speech recognition. For example, to generate new text, we can start with a seed sequence of words and use the RNN to predict the next word in the sequence. We can then append this word to the sequence and repeat the process to generate a longer sequence of text.\n",
    "\n",
    "### Text Tokenization\n",
    "Text tokenization is the process of breaking up a sequence of text into smaller units, which are typically words or subwords. This is a common preprocessing step in natural language processing (NLP) and is required because most NLP models are unable to process raw text directly.\n",
    "\n",
    "Tokenization can be done in a number of ways depending on the specific application and requirements. Some common methods include:\n",
    "\n",
    "- Word-level tokenization: This involves splitting text into individual words, with punctuation and whitespace as delimiters.\n",
    "\n",
    "- Character-level tokenization: This involves splitting text into individual characters.\n",
    "\n",
    "- Subword-level tokenization: This involves splitting text into smaller units than words, which can be useful for handling out-of-vocabulary words. One popular approach to subword tokenization is byte-pair encoding (BPE), which recursively merges the most frequent pairs of characters in a corpus.\n",
    "\n",
    "Once the text has been tokenized, the resulting sequence of tokens can be used as input to an NLP model such as a recurrent neural network (RNN) or transformer.\n",
    "\n",
    "## Sampling Novel Sequences\n",
    "Sampling novel sequences in RNNs involves using a trained language model to generate new sequences of text that have similar patterns to the original training data. This can be done by feeding a starting sequence of tokens to the RNN and allowing it to predict the next token in the sequence. This predicted token is then added to the sequence, and the process is repeated to generate a longer sequence.\n",
    "\n",
    "Sampling is usually done using a probability distribution over the predicted tokens, with the distribution being determined by the output of the RNN's softmax layer. One common approach is to use a technique called temperature scaling, where the distribution is scaled by a temperature parameter that controls the level of randomness in the generated sequence. A higher temperature results in a more diverse and unpredictable sequence, while a lower temperature produces a more deterministic sequence that closely matches the training data.\n",
    "\n",
    "Sampling is often used in natural language generation tasks, such as generating captions for images or producing dialogue in chatbots. It can also be used to generate new music or other forms of creative content.\n",
    "\n",
    "## Character-Level Language Model\n",
    "In a character-level language model, the basic unit of the model is the character. Instead of considering words as units of text, the model learns to predict the next character in a sequence based on the previous characters. For example, given the sequence \"The cat in the\", the model would predict the next character to be \" \" (a space) with a certain probability, because that is the most common next character after those characters in the training data.\n",
    "\n",
    "On the other hand, in a word-level language model, the basic unit of the model is the word. Instead of considering individual characters, the model learns to predict the next word in a sequence based on the previous words. For example, given the sequence \"The cat in the\", the model would predict the next word to be \"hat\" with a certain probability, because that is the most common next word after those words in the training data.\n",
    "\n",
    "Character-level language models are useful in cases where the language is very informal or includes many new words or word combinations that are not found in standard dictionaries. They can also be more effective in capturing the syntax and grammar of a language. However, they tend to require more training data and computational resources than word-level models, and they can also suffer from the issue of spelling variations and typos.\n",
    "\n",
    "### Exploding and Vanishing Gradients\n",
    "\n",
    "Exploding and vanishing gradients are common problems that arise in RNNs during training, and they can cause the model to converge slowly or not at all.\n",
    "\n",
    "Exploding gradients occur when the gradient values grow exponentially as they backpropagate through the network. This can lead to numeric instability and can cause the weights to update too drastically during training. One way to address this is to clip the gradients by setting a threshold on their maximum value. This is called gradient clipping, and it involves rescaling the gradient vector whenever it exceeds a certain threshold. Gradient clipping is often implemented in RNNs to prevent the gradients from becoming too large and to improve the stability of the model during training.\n",
    "\n",
    "Vanishing gradients, on the other hand, occur when the gradient values shrink exponentially as they backpropagate through the network. This can cause the weights to update very slowly or not at all, which can make it difficult for the model to learn long-term dependencies. One way to address this is to use activation functions that are less prone to vanishing gradients, such as ReLU or its variants. Additionally, a popular method for addressing vanishing gradients is to use specialized RNN architectures, such as LSTMs or GRUs, which are designed to better capture long-term dependencies. These architectures include mechanisms that selectively retain information from previous time steps, which helps prevent the gradients from vanishing as they propagate backwards through the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units\n",
    "Gated Recurrent Units (GRUs) are a type of RNN architecture that are used to handle the vanishing gradient problem in standard RNNs. GRUs were introduced by Cho et al. in their 2014 paper \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\".\n",
    "\n",
    "A GRU unit has a hidden state that is updated at each time step, and it also has two gates: an update gate and a reset gate. These gates control how much of the previous hidden state is passed to the current time step and how much of the new input is incorporated into the new hidden state.\n",
    "\n",
    "The update gate determines how much of the previous hidden state is kept and how much of the new input is used to update the hidden state. The reset gate determines how much of the previous hidden state is discarded and how much of the new input is used to reset the hidden state. By controlling the amount of information that is passed from the previous time step and the new input, the GRU architecture can handle long-term dependencies and avoid the vanishing gradient problem.\n",
    "\n",
    "In summary, GRUs are a type of RNN architecture that use update and reset gates to control how much information from the previous hidden state and the current input is used to update the current hidden state. This allows them to effectively handle long-term dependencies and avoid the vanishing gradient problem.\n",
    "\n",
    "# Long Short Term Memory\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that is designed to overcome the vanishing gradient problem in traditional RNNs, and to capture long-term dependencies in sequence data. LSTM networks are composed of memory blocks that are connected through a series of gates that allow or restrict the flow of information. Each memory block contains a cell state that can be modified or cleared through a series of operations.\n",
    "\n",
    "The three gates used in an LSTM are the input gate, forget gate, and output gate. The input gate controls the extent to which new information is allowed to enter the cell state, the forget gate controls the extent to which information is retained or discarded from the cell state, and the output gate controls the extent to which the cell state is used to influence the output. These gates are learned during training, and their activation levels are determined by the input data and the previous state of the network.\n",
    "\n",
    "The cell state is designed to allow information to flow through the network without being significantly modified or distorted. The memory blocks of the LSTM have a mechanism to control the amount of information that flows through the cell state, and they can also add or remove information from the cell state through a series of mathematical operations. This allows the LSTM to selectively retain important information and discard irrelevant or redundant information.\n",
    "\n",
    "LSTMs have shown significant improvements over traditional RNN architectures for tasks such as speech recognition, machine translation, and sentiment analysis.\n",
    "\n",
    "### Peephole Connections\n",
    "Peephole connections are a type of connection used in Long Short-Term Memory (LSTM) networks. In standard LSTMs, the cell state is updated based on the input, previous hidden state, and current memory state. Peephole connections extend this idea by allowing the current memory state to influence the gate activations directly.\n",
    "\n",
    "In a peephole LSTM, the input gate, forget gate, and output gate now include an additional connection from the memory cell. Specifically, the peephole connections allow the gates to use the current cell state directly to compute their activations. This allows the model to incorporate more information from the cell state into the gate activations.\n",
    "\n",
    "The equations for the input, forget, and output gates in a peephole LSTM are given by:\n",
    "\n",
    "Input gate: i_t = σ(W_i x_t + U_i h_{t-1} + V_i c_{t-1} + b_i)\n",
    "Forget gate: f_t = σ(W_f x_t + U_f h_{t-1} + V_f c_{t-1} + b_f)\n",
    "Output gate: o_t = σ(W_o x_t + U_o h_{t-1} + V_o c_t + b_o)\n",
    "where i_t, f_t, and o_t are the activations of the input, forget, and output gates, respectively; x_t is the input at time step t; h_{t-1} is the previous hidden state; c_{t-1} is the previous cell state; W_i, W_f, W_o, U_i, U_f, U_o, V_i, V_f, and V_o are weight matrices; b_i, b_f, b_o are bias terms; and σ is the sigmoid activation function.\n",
    "\n",
    "The peephole LSTM has been shown to improve performance on certain tasks compared to standard LSTMs.\n",
    "\n",
    "## Bidirectional RNNs\n",
    "\n",
    "In a unidirectional RNN, the output sequence is generated based on the past input sequence. This means that each output depends only on the previous elements of the input sequence. In some applications, however, the output at a given time step may depend on both past and future elements of the input sequence. Bidirectional algorithms are designed to deal with this issue.\n",
    "\n",
    "A bidirectional RNN (BiRNN) consists of two RNNs, one processing the input sequence forward and the other processing it backward. The two RNNs are independent, but they share the same output layer. The idea is that each output at a given time step is influenced by both past and future elements of the input sequence.\n",
    "\n",
    "For example, in a language modeling task, the next word in a sentence may depend not only on the preceding words, but also on the words that come after it. Bidirectional algorithms can be used to capture these dependencies and improve the performance of the model.\n",
    "\n",
    "## Deep RNNs\n",
    "Deep RNNs are an extension of standard RNNs that incorporate multiple layers of recurrent units, allowing for more complex and abstract representations to be learned from sequential data. Each layer in a deep RNN typically includes its own set of recurrent connections and output connections, and the output of one layer is fed as input to the next layer. By incorporating multiple layers, deep RNNs can capture hierarchical patterns and dependencies in sequential data that may not be possible with a single-layer RNN. However, training deep RNNs can be challenging due to the potential for vanishing or exploding gradients, and careful initialization and regularization techniques are often necessary to ensure convergence during training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
