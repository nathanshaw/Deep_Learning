{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaNet5\n",
    "The LeNet-5 architecture is a convolutional neural network (CNN) designed by Yann LeCun in 1998, and it was one of the earliest successful CNN models for image classification. It consists of 7 layers, including 3 convolutional layers, 2 subsampling (pooling) layers, and 2 fully connected layers. Here's a brief overview of each layer:\n",
    "- Convolutional layer 1: This layer has 6 filters of size 5x5 and applies a rectified linear unit (ReLU) activation function. It also has a max pooling layer that reduces the dimensionality by half.\n",
    "- Convolutional layer 2: This layer has 16 filters of size 5x5 and applies a ReLU activation function. It also has a max pooling layer that reduces the dimensionality by half.\n",
    "- Convolutional layer 3: This layer has 120 filters of size 5x5 and applies a ReLU activation function.\n",
    "- Fully connected layer 1: This layer has 84 neurons and applies a ReLU activation function.\n",
    "- Fully connected layer 2: This layer has 10 neurons (for 10 classes in the MNIST dataset) and applies a softmax activation function.\n",
    "\n",
    "The LeNet-5 architecture is relatively simple compared to more modern CNN architectures, but it was groundbreaking at the time of its creation and has inspired many subsequent CNN models. It is particularly well-suited to the MNIST dataset, which consists of 28x28 grayscale images of handwritten digits, but it can also be adapted to other image classification tasks with some modifications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "The AlexNet architecture is a convolutional neural network (CNN) designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. It was the winner of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, and it was one of the first deep neural networks to achieve state-of-the-art performance on large-scale image recognition tasks. The architecture consists of 8 layers, including 5 convolutional layers and 3 fully connected layers. Here's a brief overview of each layer:\n",
    "- Convolutional layer 1: This layer has 96 filters of size 11x11 with a stride of 4 and applies a rectified linear unit (ReLU) activation function. It also has a local response normalization layer that normalizes the outputs of neighboring neurons.\n",
    "- Convolutional layer 2: This layer has 256 filters of size 5x5 with a stride of 1 and applies a ReLU activation function. It also has a local response normalization layer.\n",
    "- Convolutional layer 3: This layer has 384 filters of size 3x3 with a stride of 1 and applies a ReLU activation function.\n",
    "- Convolutional layer 4: This layer has 384 filters of size 3x3 with a stride of 1 and applies a ReLU activation function.\n",
    "- Convolutional layer 5: This layer has 256 filters of size 3x3 with a stride of 1 and applies a ReLU activation function. It also has a max pooling layer that reduces the dimensionality by half.\n",
    "- Fully connected layer 1: This layer has 4096 neurons and applies a ReLU activation function. It also has a dropout layer that randomly drops out some of the neurons during training to prevent overfitting.\n",
    "- Fully connected layer 2: This layer has 4096 neurons and applies a ReLU activation function. It also has a dropout layer.\n",
    "- Fully connected layer 3: This layer has 1000 neurons (for 1000 classes in the ImageNet dataset) and applies a softmax activation function.\n",
    "\n",
    "The AlexNet architecture introduced several innovations, including the use of ReLU activation functions, local response normalization, and dropout, that helped to improve the performance of deep neural networks on large-scale image recognition tasks. It also demonstrated the importance of deep architectures and parallel processing on GPUs for training these models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16\n",
    "The VGG-16 network is a convolutional neural network (CNN) architecture designed by the Visual Geometry Group (VGG) at the University of Oxford. It was proposed by Karen Simonyan and Andrew Zisserman in 2014 and achieved state-of-the-art performance on the ImageNet dataset. Here's a brief overview of the architecture:\n",
    "\n",
    "- Input layer: This layer takes an input image of size 224x224x3.\n",
    "- Convolutional layers: The network consists of 13 convolutional layers, each with a 3x3 filter size and a stride of 1. All of the convolutional layers use the rectified linear unit (ReLU) activation function. The first two convolutional layers have 64 filters, and the remaining 11 convolutional layers have 128 filters.\n",
    "- Max pooling layers: After each set of convolutional layers, there is a max pooling layer that reduces the spatial dimension of the feature map by a factor of 2.\n",
    "- Fully connected layers: The network consists of 3 fully connected layers, each with 4,096 neurons and a ReLU activation function. The final fully connected layer has 1,000 neurons (corresponding to the number of classes in the ImageNet dataset) and uses a softmax activation function.\n",
    "\n",
    "The VGG-16 architecture is characterized by its simplicity and depth, with 16 layers (hence the name \"VGG-16\") and small 3x3 filters used throughout the network. This architecture allows the network to learn more complex features than networks with larger filters, while also keeping the number of parameters relatively low. The VGG-16 network has been widely used as a starting point for many other CNN architectures and has helped to establish deep learning as a key technique in computer vision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNets\n",
    "A ResNet (short for \"Residual Network\") is a type of convolutional neural network (CNN) architecture that was introduced by Kaiming He et al. in 2015. The ResNet architecture is designed to help address the problem of vanishing gradients, which can occur when training very deep neural networks.\n",
    "\n",
    "In a ResNet, each layer is connected not only to the layer that follows it, but also to the layer several steps ahead. This type of connection is called a \"skip connection\" or \"shortcut connection,\" and it allows information to bypass one or more layers that might otherwise lead to vanishing gradients. Specifically, the ResNet architecture uses residual connections, which add the input of a layer to its output, allowing the layer to learn the residual (i.e., the difference between the output and the input) rather than the full mapping.\n",
    "\n",
    "Here's how a residual connection works: suppose we have a convolutional layer F(x) that takes an input x and produces an output y=F(x). The residual connection adds the input x to the output y, producing the final output z=x+y. This approach allows the network to learn the residual mapping F(x) rather than the full mapping H(x) = F(x) + x, which can be easier to optimize and less prone to vanishing gradients.\n",
    "\n",
    "In addition to helping to mitigate vanishing gradients, residual connections also make it easier to train very deep neural networks by allowing gradients to flow more directly from the output layer to the input layer. This can help to prevent the problem of exploding gradients, which can occur when gradients get too large and cause the weights to update too aggressively.\n",
    "\n",
    "Overall, the ResNet architecture has proven to be highly effective for a wide range of computer vision tasks, including image classification, object detection, and semantic segmentation.\n",
    "\n",
    "## Residual block\n",
    "A residual block consists of a sequence of convolutional layers, followed by batch normalization and activation functions (usually ReLU). The output of the block is the sum of the input to the block and the output of the convolutional layers. This sum is then passed through another activation function, such as ReLU.\n",
    "\n",
    "The residual block is designed to learn the residual mapping between the input and output of the block. In other words, instead of trying to learn the entire mapping between the input and output, the block learns to model the difference between the two. By using residual blocks in a network, the network can learn to approximate complex functions with greater ease, as the blocks can help to propagate the gradients more effectively throughout the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1 convolutions\n",
    "A 1x1 convolution is a type of convolutional layer used in convolutional neural networks (CNNs). Unlike standard convolutional layers that use filters with a larger spatial extent (e.g., 3x3 or 5x5), 1x1 convolutions use filters with a spatial extent of 1x1 pixels.\n",
    "\n",
    "Technically speaking, a 1x1 convolution is just a convolutional layer with a kernel size of 1x1. When applied to a feature map, a 1x1 convolution computes a weighted sum of the input values within each 1x1 local neighborhood, using a set of learnable weights (i.e., the filter weights) and a bias term. The output of a 1x1 convolution is a new feature map with the same spatial dimensions as the input, but with a potentially different number of channels (i.e., depth).\n",
    "\n",
    "One of the main advantages of using 1x1 convolutions is that they can reduce the number of channels in a feature map without affecting its spatial dimensions. By using a set of 1x1 convolutions to reduce the number of channels in a feature map, we can reduce the computational cost of the network and prevent overfitting. 1x1 convolutions can also introduce non-linearities into the network and capture interactions between different feature maps.\n",
    "\n",
    "1x1 convolutions are commonly used in a variety of computer vision tasks, including image classification, object detection, and semantic segmentation. They are particularly useful in deep neural networks, where they can help to reduce the number of parameters and improve the computational efficiency of the network.\n",
    "\n",
    "1x1 convolutions, also known as network-in-network (NiN) layers, are an important technique in convolutional neural networks (CNNs) that can help to improve their performance and efficiency.\n",
    "\n",
    "Here are a few ways in which 1x1 convolutions can be useful in machine learning:\n",
    "\n",
    "- Dimensionality reduction: 1x1 convolutions can be used to reduce the number of feature maps in a CNN, which can help to reduce the computational cost of the network and prevent overfitting. By applying a set of 1x1 convolutions to the output of a convolutional layer, we can reduce the depth (number of channels) of the feature maps, while preserving their spatial dimensions. This can help to reduce the number of parameters in the network and make it more efficient.\n",
    "- Non-linear transformations: 1x1 convolutions can be used to introduce non-linearities into the network. By applying a non-linear activation function (such as ReLU) to the output of a set of 1x1 convolutions, we can introduce non-linearities into the network that can help to improve its expressive power.\n",
    "- Feature interactions: 1x1 convolutions can be used to capture interactions between different feature maps in the network. By applying a set of 1x1 convolutions to the output of a convolutional layer, we can compute cross-channel interactions between the feature maps, which can help to improve the discriminative power of the network.\n",
    "\n",
    "Overall, 1x1 convolutions are a powerful tool for improving the performance and efficiency of CNNs, and they are commonly used in a variety of computer vision tasks, including image classification, object detection, and semantic segmentation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Network\n",
    "Sure! The Inception network is a convolutional neural network (CNN) architecture that was developed by Google researchers in 2014. The main idea behind the Inception network is to create a network that can efficiently learn a wide range of feature maps at different scales and resolutions, while minimizing the computational cost and number of parameters.\n",
    "\n",
    "The Inception network consists of multiple Inception modules, each of which contains a set of convolutional layers with different filter sizes (1x1, 3x3, and 5x5), as well as a pooling layer. The outputs of these different filters are concatenated together and passed through another set of convolutional layers to produce the final output of the module. By using filters with different sizes, the Inception network can capture features at different scales and resolutions, which can help to improve its performance on a wide range of computer vision tasks.\n",
    "\n",
    "Here's a simplified example of an Inception module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(inputs, n_filters):\n",
    "    # 1x1 convolution branch\n",
    "    branch1 = Conv2D(n_filters[0], (1, 1), padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    # 3x3 convolution branch\n",
    "    branch2 = Conv2D(n_filters[1], (1, 1), padding='same', activation='relu')(inputs)\n",
    "    branch2 = Conv2D(n_filters[2], (3, 3), padding='same', activation='relu')(branch2)\n",
    "    \n",
    "    # 5x5 convolution branch\n",
    "    branch3 = Conv2D(n_filters[3], (1, 1), padding='same', activation='relu')(inputs)\n",
    "    branch3 = Conv2D(n_filters[4], (5, 5), padding='same', activation='relu')(branch3)\n",
    "    \n",
    "    # max pooling branch\n",
    "    branch4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch4 = Conv2D(n_filters[5], (1, 1), padding='same', activation='relu')(branch4)\n",
    "    \n",
    "    # concatenate outputs from all branches\n",
    "    output = concatenate([branch1, branch2, branch3, branch4], axis=-1)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the Inception module takes an input tensor and outputs a tensor with a larger number of feature maps. The module consists of four branches: a 1x1 convolution branch, a 3x3 convolution branch, a 5x5 convolution branch, and a max pooling branch. Each branch produces a set of feature maps with a different number of channels, which are then concatenated together along the channel axis to produce the final output of the module.\n",
    "\n",
    "The Inception network has proven to be highly effective for a wide range of computer vision tasks, including image classification, object detection, and semantic segmentation. Its ability to efficiently learn feature maps at different scales and resolutions has made it a popular choice for deep learning practitioners."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet\n",
    "MobileNet is a convolutional neural network (CNN) architecture that was designed for mobile and embedded devices, where computational resources are limited. The main idea behind MobileNet is to create a network that is both lightweight and efficient, while still achieving high accuracy on a variety of computer vision tasks.\n",
    "\n",
    "The MobileNet architecture is based on a type of convolutional layer called depthwise separable convolutions, which consists of two main components: a depthwise convolution and a pointwise convolution. The depthwise convolution applies a single filter to each input channel, and the pointwise convolution applies a set of 1x1 filters to the output of the depthwise convolution to produce the final output. By using depthwise separable convolutions, the MobileNet architecture is able to reduce the number of parameters and computational cost of the network, while still achieving high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobile_block(inputs, filters, alpha=1):\n",
    "    # depthwise convolution\n",
    "    x = DepthwiseConv2D((3, 3), padding='same', depth_multiplier=alpha)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # pointwise convolution\n",
    "    x = Conv2D(int(filters * alpha), (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the MobileNet block takes an input tensor and applies a depthwise convolution followed by a pointwise convolution. The depthwise convolution uses a small filter size (3x3) and a small number of channels (depth_multiplier=alpha) to reduce the computational cost of the network. The pointwise convolution uses a set of 1x1 filters to increase the number of channels in the output feature maps.\n",
    "\n",
    "MobileNet is commonly used for a variety of computer vision tasks, including image classification, object detection, and semantic segmentation. Its lightweight and efficient architecture makes it well-suited for mobile and embedded devices, where computational resources are limited. It is also used in transfer learning, where it can be fine-tuned on smaller datasets to achieve high accuracy with limited computational resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet\n",
    "EfficientNet is a family of convolutional neural network (CNN) architectures that were designed to achieve state-of-the-art accuracy on image classification tasks, while using fewer computational resources than existing CNN architectures.\n",
    "\n",
    "The EfficientNet architecture is based on a novel scaling method that uniformly scales the depth, width, and resolution of the network. Specifically, the EfficientNet architecture consists of multiple blocks, each of which contains a combination of convolutional layers, skip connections, and a squeeze-and-excitation (SE) block for feature recalibration. The depth, width, and resolution of the network are scaled by applying a set of scaling coefficients to the number of filters, the number of layers, and the input resolution of each block, respectively.\n",
    "\n",
    "Here's a simplified example of an EfficientNet block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_block(inputs, filters, strides=1):\n",
    "    # 1x1 convolution\n",
    "    x = Conv2D(filters, 1, padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    \n",
    "    # depthwise convolution\n",
    "    x = DepthwiseConv2D(3, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    \n",
    "    # squeeze-and-excitation block\n",
    "    se = GlobalAveragePooling2D()(x)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Conv2D(int(filters * 0.25), 1, padding='same', activation='swish', use_bias=False)(se)\n",
    "    se = Conv2D(filters, 1, padding='same', activation='sigmoid', use_bias=False)(se)\n",
    "    x = Multiply()([x, se])\n",
    "    \n",
    "    # skip connection\n",
    "    if strides == 1 and inputs.shape[-1] == filters:\n",
    "        x = Add()([x, inputs])\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the EfficientNet block takes an input tensor and applies a combination of a 1x1 convolution, a depthwise convolution, a squeeze-and-excitation block, and a skip connection. The 1x1 convolution and depthwise convolution are used to learn feature maps at different resolutions, while the squeeze-and-excitation block is used to recalibrate the feature maps by learning the importance of each channel. The skip connection is used to preserve information from the input tensor and prevent the gradients from vanishing during training.\n",
    "\n",
    "EfficientNet is commonly used for image classification tasks, such as recognizing objects in images or identifying facial expressions. Its ability to achieve high accuracy while using fewer computational resources has made it popular in real-world applications where computational resources are limited, such as mobile devices and embedded systems. The different variants of EfficientNet (e.g., EfficientNet-B0, EfficientNet-B1, etc.) provide a range of trade-offs between accuracy and computational efficiency, allowing practitioners to choose the architecture that best fits their needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depthwise Separable Convolution\n",
    "Depthwise Separable Convolution is a type of convolutional layer used in deep neural networks that factorizes a standard convolution into two separate operations: a depthwise convolution and a pointwise convolution.\n",
    "\n",
    "In a standard convolution, a kernel (a small matrix of weights) is convolved with an input image to produce a feature map. This process requires a large number of computations and parameters, especially if the input and kernel are large. Depthwise Separable Convolution reduces the number of parameters and computations by dividing the convolution into two steps:\n",
    "\n",
    "- **Depthwise Convolution**: The depthwise convolution applies a separate filter to each input channel (i.e., each color channel in an RGB image) and produces a set of feature maps. This step is called \"depthwise\" because it operates on the depth (i.e., number of input channels) of the input.\n",
    "\n",
    "- **Pointwise Convolution**: The pointwise convolution applies a 1x1 convolution (i.e., a convolution with a kernel size of 1x1) to the output of the depthwise convolution. This operation combines the output channels of the depthwise convolution into a smaller set of feature maps. This step is called \"pointwise\" because it operates on each pixel (i.e., point) in the output of the depthwise convolution.\n",
    "\n",
    "By separating the depthwise and pointwise convolutions, Depthwise Separable Convolution reduces the number of parameters and computations required by the convolution. This can make the convolutional layers more computationally efficient and effective, especially when dealing with large images or complex neural networks.\n",
    "\n",
    "Depthwise Separable Convolution is used in many state-of-the-art deep neural network architectures, including MobileNet and EfficientNet, to improve performance while reducing computational complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expansion and Projection Layers\n",
    "In the MobileNet V2 architecture, the expansion layer and projection layer are two important components of the bottleneck block. The bottleneck block is used to reduce the computational complexity of the network while maintaining high accuracy.\n",
    "\n",
    "The expansion layer is the first layer in the bottleneck block and is responsible for increasing the number of channels in the feature maps. It does this by applying a 1x1 convolution to the input feature maps. The number of output channels is typically larger than the number of input channels, which helps to increase the expressive power of the network. The purpose of the expansion layer is to provide a non-linear transformation that can capture complex patterns in the input data.\n",
    "\n",
    "After the expansion layer, the projection layer reduces the number of channels back to the original dimension. This is achieved by applying a 1x1 convolution to the output of the expansion layer. The purpose of the projection layer is to reduce the computational cost of subsequent layers while retaining the important features captured by the expansion layer. By reducing the number of channels, the projection layer also helps to reduce the memory requirements of the network.\n",
    "\n",
    "Overall, the expansion and projection layers in the MobileNet V2 architecture work together to create a bottleneck block that can capture complex patterns in the input data while maintaining a low computational cost. The bottleneck block is repeated multiple times in the network to create a deep and efficient neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
